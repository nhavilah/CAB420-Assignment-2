{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels import api as sm\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn import decomposition\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard import notebook\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import decomposition\n",
    "from sklearn import discriminant_analysis\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy.io\n",
    "import numpy\n",
    "import cv2\n",
    "\n",
    "from sklearn.svm import SVC,NuSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_curve,auc\n",
    "def eval_model(model):\n",
    "    #predict Test Accuracy\n",
    "    test_scores = model.evaluate(test,test_y,verbose=2)\n",
    "    print(\"Testing Loss: \"+str(test_scores[0]))\n",
    "    print(\"Testing Accuracy: \"+str(test_scores[1]))\n",
    "\n",
    "    #display the ROC Curve\n",
    "    classes = numpy.unique(train_y)\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    for i in range(10):\n",
    "        fpr,tpr,_=roc_curve(test_y,pred[:,i],pos_label=classes[i])\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        ax.plot(fpr,tpr,label='%s: %0.2f' % (classes[i],auc_score))\n",
    "    ax.legend()\n",
    "\n",
    "    #confusion matrix\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    pred = model.predict(test)\n",
    "    indexes = tf.argmax(pred,axis=1)\n",
    "    cm = confusion_matrix(test_y,indexes)\n",
    "    c = ConfusionMatrixDisplay(cm,display_labels=range(10))\n",
    "    c.plot(ax=ax)\n",
    "    ax.set_title('Testing Performance')\n",
    "    eval_model(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\Anaconda3\\envs\\Cab\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30607, 7560)\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.abspath(os.getcwd())+\"/256_ObjectCategories/\"\n",
    "#add the arrays that we can use for training and testing\n",
    "main_x = []\n",
    "main_y = []\n",
    "prev_file = [\"\"]\n",
    "current_index = 0\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for dir in dirs:\n",
    "        current_dir = os.path.join(root_dir) + dir\n",
    "        after = str(current_dir.split(\".\",1)[1])\n",
    "        for file in os.listdir(current_dir):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                current_file = cv2.imread(os.path.join(current_dir,filename),0)\n",
    "                current_file = cv2.resize(current_file, dsize=(54, 140), interpolation=cv2.INTER_CUBIC)\n",
    "                color_features = current_file.flatten()\n",
    "                grey_image = rgb2gray(current_file)\n",
    "                hog_features = hog(grey_image,block_norm='L2-Hys',pixels_per_cell=(16,16))\n",
    "                flat_features = np.hstack(color_features)\n",
    "                main_x.append(flat_features)\n",
    "                main_y.append(current_index)\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        current_index = current_index+1\n",
    "    break\n",
    "images_x = np.array(main_x)\n",
    "images_y = np.array(main_y)\n",
    "print(np.shape(images_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118 127 143 ... 126 131 124]\n"
     ]
    }
   ],
   "source": [
    "print(images_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19588, 7560)\n",
      "(6122, 7560)\n",
      "(4897, 7560)\n"
     ]
    }
   ],
   "source": [
    "temp_x_train, x_test, temp_y_train, y_test = train_test_split(images_x,images_y,test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(temp_x_train,temp_y_train,test_size=0.2,random_state=42)\n",
    "print(numpy.shape(x_train))\n",
    "print(numpy.shape(x_test))\n",
    "print(numpy.shape(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-40dcf7512027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFcAAABRCAYAAACnkTpxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcklEQVR4nO2cwYtVZRiHn5+mm9m0cKDIJlqIg+3sIrlzI6gEbVzoRohgMOoPaJX/QxDJLETaKK7EheK2FgXORIoWwhBEQ0FYYEhRDPxa3GPcGY5zvup775kz8z5w4d57Ps59eTgc5p3feT/ZJolhV98FbGdSbiApN5CUG0jKDSTlBtIpV9IlST9Luv+M45L0kaQVSfckHa5f5jApuXIvAyc2OX4SONC8FoBP/n9Z24NOubY/A37dZMlbwKce8yXwvKQXaxU4ZGrcc18Cfpj4vNp8t+N5rsI51PJda08taYHxrYOZmZnX5+fnK/z89FleXn5ke7ZrXQ25q8DLE5/3Az+2LbS9CCwCjEYjLy0tVfj56SPp+5J1NW4LN4BzzV8NbwCPbf9U4byDp/PKlXQFOAbsk7QKXAD2ANi+CNwETgErwO/A21HFDo1OubbPdhw38F61irYR2aEFknIDSbmBpNxAUm4gKTeQlBtIyg0k5QaScgNJuYGk3EBSbiApN5AiuZJOSHrYxOcftBw/JumxpK+b14f1Sx0eJf8s3w18DBxnHOnckXTD9jcbln5u+82AGgdLyZV7BFix/Z3tv4CrjOP0pIMSuaXR+VFJdyXdkvRaleoGTkn6WxKdfwW8YvuJpFPAdcZP4Kw/0US0Pjc39+8qHSAlV25ndG77N9tPmvc3gT2S9m08ke1F2yPbo9nZzth/8JTIvQMckPSqpL3AGcZx+j9IekGSmvdHmvP+UrvYoVGS/q5Jeh+4DewGLtl+IOl8c/wicBp4V9Ia8AdwxjnJgvpyMPAnbpZtj7rWZYcWSMoNJOUGknIDSbmBpNxAUm4gKTeQlBtIyg0k5QaScgNJuYGk3EBqRes5ud5CyZYAT6P1k8Ah4KykQxuW5eR6C7Wi9Zxcb6FWtJ6T6y3UitaLJtcno3Xgz2ftPjIADpYsKpFbMpVeNLk+ObUuaakkh9qKSCoK/6pE6+Tkeiu1ovWcXG+ht2hd0kJzmxgcpbX3JncnkO1vIL3I7WqntypdG9htZOpyC9vprcplNt/Abh19XLmDfVK9YAO7dfQhd8e0yn3ILd7kbej0Ibd4k7eh04fcknZ6WzB1ubbXgKft9LfANdsPpl3Hf6HZwO4L4KCkVUnvbLo+O7Q4skMLJOUGknIDSbmBpNxAUm4gKTeQlBvI3zG8LMxYf4bgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(images_y.dtype)\n",
    "# fig = plt.figure(figsize=[10,10])\n",
    "# for i in range(100):\n",
    "#     ax = fig.add_subplot(10,10,i+1)\n",
    "#     ax.imshow(x_train[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#THIS DATA IS ONLY FOR THE CNN, IT WONT WORK FOR KNN OR SVM\n",
    "root_dir = os.path.abspath(os.getcwd())+\"/256_ObjectCategories/\"\n",
    "#add the arrays that we can use for training and testing\n",
    "main_x = []\n",
    "main_y = []\n",
    "prev_file = [\"\"]\n",
    "current_index = 0\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for dir in dirs:\n",
    "        current_dir = os.path.join(root_dir) + dir\n",
    "        after = str(current_dir.split(\".\",1)[1])\n",
    "        for file in os.listdir(current_dir):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                current_file = cv2.imread(os.path.join(current_dir,filename),0)\n",
    "                current_file = cv2.resize(current_file, dsize=(54, 140), interpolation=cv2.INTER_CUBIC)\n",
    "                main_x.append(current_file)\n",
    "                main_y.append(current_index)\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        current_index = current_index+1\n",
    "    break\n",
    "images_x = np.array(main_x)\n",
    "images_y = np.array(main_y)\n",
    "print(np.shape(images_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30607,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(images_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30607, 256)\n"
     ]
    }
   ],
   "source": [
    "#apply PCA and dimension reduction to the data\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(images_x)\n",
    "transformed = pca.transform(images_x)\n",
    "lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda.fit(transformed,images_y)\n",
    "new_data = lda.transform(transformed)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30607,)\n"
     ]
    }
   ],
   "source": [
    "print(images_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19588, 256)\n",
      "(6122, 256)\n",
      "(4897, 256)\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "temp_x_train, x_test, temp_y_train, y_test = train_test_split(new_data,images_y,test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(temp_x_train,temp_y_train,test_size=0.2,random_state=42)\n",
    "print(numpy.shape(x_train))\n",
    "print(numpy.shape(x_test))\n",
    "print(numpy.shape(x_val))\n",
    "print(images_y[0])\n",
    "print(images_y[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\Anaconda3\\envs\\Cab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 6 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 2\n",
      "Best n_neighbors: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2206795164978765"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "def get_compiled_model():knn_clf = KNeighborsClassifier()\n",
    "    param_distributions = {\"leaf_size\": uniform(1,20),\"n_neighbors\": uniform(1,20),\"p\":uniform(1,2)}\n",
    "    rnd_search_cv = RandomizedSearchCV(knn_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
    "    rnd_search_cv.fit(x_train, y_train)\n",
    "    rnd_search_cv.best_estimator_\n",
    "    rnd_search_cv.best_score_\n",
    "    return rnd_search_cv.best_estimator_\n",
    "\n",
    "def make_or_restore_model():\n",
    "    #either restore the latest model or create a fresh one if theres no checkpoint available\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints,key=os.path.getctime)\n",
    "        print(\"restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "def run_training():\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = make_or_restore_model()\n",
    "    callbacks = [\n",
    "        # this callback saves a saved model every epoch\n",
    "        # we can include the current epoch in the folder name\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_dir + \"/ckpt-{epoch}\",save_freq=\"epoch\"\n",
    "        )\n",
    "    ]\n",
    "    model.fit(x_train,y_train)\n",
    "    return model\n",
    "\n",
    "#the first run creates the model\n",
    "run_training()\n",
    "\n",
    "#the second time you call it will pick off from the last saved point\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup the SVM\n",
    "#grid search cross validation has already been imported ready for hyperparameter tuning\n",
    "svm = SVC(class_weight='balanced')\n",
    "#add some stuff for some shitty hyperparameter tuning here. You guys figure out the smarter way to do it. I belive the smarter way is using the range functionality to narrow down the range of values but you will need to investigate\n",
    "#param_grid = [\n",
    "#    {'C':[0.1,1,10,100],'kernel':['linear']},\n",
    "#    {'C':[0.1,1,10,100],'gamma':[0.1,0.01,0.001],'kernel':['rbf']},\n",
    "#    {'C':[0.1,1,10,100],'degree':[3,4,5],'kernel':['poly']},\n",
    "#]\n",
    "#grid_search = GridSearchCV(svm, param_grid)\n",
    "#grid_search.fit(x_train,y_train)\n",
    "#grid_search.cv_results_\n",
    "#best_system = np.argmin(grid_search.cv_results_['rank_test_score'])\n",
    "#params = grid_search.cv_results_['params'][best_system]\n",
    "#print(params)\n",
    "#svm = SVC().set_params(**params)\n",
    "svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Creating a new model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reciprocal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-29f3bf9c26f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#the first run creates the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#the second time you call it will pick off from the last saved point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-29f3bf9c26f8>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_or_restore_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     callbacks = [\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# this callback saves a saved model every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-29f3bf9c26f8>\u001b[0m in \u001b[0;36mmake_or_restore_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating a new model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_compiled_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-29f3bf9c26f8>\u001b[0m in \u001b[0;36mget_compiled_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msvm_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"scale\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"gamma\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreciprocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reciprocal' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "def get_compiled_model():\n",
    "    svm_clf = SVC(gamma=\"scale\")\n",
    "\n",
    "    param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "    rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
    "    rnd_search_cv.fit(x_train, y_train)\n",
    "    rnd_search_cv.best_estimator_\n",
    "    rnd_search_cv.best_score_\n",
    "    return rnd_search_cv.best_estimator_\n",
    "\n",
    "def make_or_restore_model():\n",
    "    #either restore the latest model or create a fresh one if theres no checkpoint available\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints,key=os.path.getctime)\n",
    "        print(\"restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "def run_training():\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = make_or_restore_model()\n",
    "    callbacks = [\n",
    "        # this callback saves a saved model every epoch\n",
    "        # we can include the current epoch in the folder name\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_dir + \"/ckpt-{epoch}\",save_freq=\"epoch\"\n",
    "        )\n",
    "    ]\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "#the first run creates the model\n",
    "run_training()\n",
    "\n",
    "#the second time you call it will pick off from the last saved point\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 140, 54, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 140, 54, 8)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 140, 54, 8)        584       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 140, 54, 8)        32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 140, 54, 8)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 140, 54, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 70, 27, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 70, 27, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 70, 27, 8)         584       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 70, 27, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 70, 27, 8)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 70, 27, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 35, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 35, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 35, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 35, 13, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 35, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_5 (Spatial (None, 35, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3640)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               932096    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 257)               16705     \n",
      "=================================================================\n",
      "Total params: 968,345\n",
      "Trainable params: 968,297\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "def build_model(num_classes,output_activation=None):\n",
    "    inputs = keras.Input(shape=(140,54,1),name='img')\n",
    "    x = layers.Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.35)(x) #removes some of the learned features\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(4, 4))(x) #makes images softer to remove slight changes to reduce feature maps - downsampling and keeps details\n",
    "    \n",
    "    x = layers.Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.35)(x) #removes some of the learned features\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(4, 4))(x) #makes images softer to remove slight changes to reduce feature maps - downsampling and keeps details\n",
    "    \n",
    "    x = layers.Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.35)(x) #removes some of the learned features\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(4, 4))(x) #makes images softer to remove slight changes to reduce feature maps - downsampling and keeps details\n",
    "    \n",
    "    x = layers.Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.35)(x) #removes some of the learned features\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(4, 4))(x) #makes images softer to remove slight changes to reduce feature maps - downsampling and keeps details\n",
    "    \n",
    "    x = layers.Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=None)(x)\n",
    "    # batch normalisation, before the non-linearity\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    # spatial dropout, this will drop whole kernels, i.e. 20% of our 3x3 filters will be dropped out rather\n",
    "    # than dropping out 20% of the invidual pixels\n",
    "    x = layers.SpatialDropout2D(0.35)(x) #removes some of the learned features\n",
    "    # max pooling, 2x2, which will downsample the image\n",
    "    x = layers.MaxPool2D(pool_size=(4, 4))(x) #makes images softer to remove slight changes to reduce feature maps - downsampling and keeps details\n",
    "    # rinse and repeat with 2D convs, batch norm, dropout and max pool\n",
    "\n",
    "    # flatten layer\n",
    "    x = layers.Flatten()(x)\n",
    "    # we'll use a couple of dense layers here, mainly so that we can show what another dropout layer looks like \n",
    "    # in the middle\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    # the output\n",
    "    outputs = layers.Dense(258,activation=output_activation)(x)\n",
    "\n",
    "    # build the model, and print a summary\n",
    "    model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='cnn_model')\n",
    "\n",
    "    return model_cnn\n",
    "\n",
    "model_cnn = build_model(257)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "97/97 [==============================] - 218s 2s/step - loss: 12.7732 - accuracy: 0.0095 - val_loss: 13.6953 - val_accuracy: 0.0265\n",
      "Epoch 2/50\n",
      "97/97 [==============================] - 200s 2s/step - loss: 13.7586 - accuracy: 0.0221 - val_loss: 13.4132 - val_accuracy: 0.0199\n",
      "Epoch 3/50\n",
      "97/97 [==============================] - 217s 2s/step - loss: 13.7847 - accuracy: 0.0154 - val_loss: 13.8386 - val_accuracy: 0.0212\n",
      "Epoch 4/50\n",
      "97/97 [==============================] - 198s 2s/step - loss: 14.0584 - accuracy: 0.0155 - val_loss: 13.8053 - val_accuracy: 0.0222\n",
      "Epoch 5/50\n",
      "97/97 [==============================] - 232s 2s/step - loss: 14.2489 - accuracy: 0.0287 - val_loss: 16.2949 - val_accuracy: 0.0114\n",
      "Epoch 6/50\n",
      "97/97 [==============================] - 232s 2s/step - loss: 12.0226 - accuracy: 0.0072 - val_loss: 5.5491 - val_accuracy: 0.0049\n",
      "Epoch 7/50\n",
      "97/97 [==============================] - 218s 2s/step - loss: 5.5503 - accuracy: 0.0034 - val_loss: 5.5491 - val_accuracy: 0.0042\n",
      "Epoch 8/50\n",
      "97/97 [==============================] - 219s 2s/step - loss: 5.5510 - accuracy: 0.0037 - val_loss: 5.5491 - val_accuracy: 0.0046\n",
      "Epoch 9/50\n",
      "97/97 [==============================] - 220s 2s/step - loss: 5.5515 - accuracy: 0.0024 - val_loss: 5.5491 - val_accuracy: 9.8007e-04\n",
      "Epoch 10/50\n",
      "97/97 [==============================] - 232s 2s/step - loss: 5.5510 - accuracy: 0.0037 - val_loss: 5.5491 - val_accuracy: 9.8007e-04\n",
      "Epoch 11/50\n",
      "97/97 [==============================] - 211s 2s/step - loss: 5.5497 - accuracy: 0.0035 - val_loss: 5.5491 - val_accuracy: 9.8007e-04\n",
      "Epoch 12/50\n",
      "97/97 [==============================] - 212s 2s/step - loss: 5.5495 - accuracy: 0.0035 - val_loss: 5.5491 - val_accuracy: 9.8007e-04\n",
      "Epoch 13/50\n",
      "97/97 [==============================] - 234s 2s/step - loss: 5.5500 - accuracy: 0.0028 - val_loss: 5.5491 - val_accuracy: 9.8007e-04\n",
      "Epoch 14/50\n",
      "97/97 [==============================] - 222s 2s/step - loss: 5.5494 - accuracy: 0.0027 - val_loss: 5.5491 - val_accuracy: 0.0039\n",
      "Epoch 15/50\n",
      "97/97 [==============================] - 216s 2s/step - loss: 5.5491 - accuracy: 0.0033 - val_loss: 5.5491 - val_accuracy: 0.0039\n",
      "Epoch 16/50\n",
      "97/97 [==============================] - 201s 2s/step - loss: 5.5500 - accuracy: 0.0038 - val_loss: 5.5491 - val_accuracy: 0.0046\n",
      "Epoch 17/50\n",
      "97/97 [==============================] - 18530s 193s/step - loss: 5.5504 - accuracy: 0.0032 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 18/50\n",
      "97/97 [==============================] - 224s 2s/step - loss: 5.5492 - accuracy: 0.0025 - val_loss: 5.5491 - val_accuracy: 0.0033\n",
      "Epoch 19/50\n",
      "97/97 [==============================] - 233s 2s/step - loss: 5.5498 - accuracy: 0.0037 - val_loss: 5.5491 - val_accuracy: 0.0033\n",
      "Epoch 20/50\n",
      "97/97 [==============================] - 228s 2s/step - loss: 5.5491 - accuracy: 0.0034 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 21/50\n",
      "97/97 [==============================] - 225s 2s/step - loss: 5.5491 - accuracy: 0.0030 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 22/50\n",
      "97/97 [==============================] - 225s 2s/step - loss: 5.5491 - accuracy: 0.0036 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 23/50\n",
      "97/97 [==============================] - 229s 2s/step - loss: 5.5491 - accuracy: 0.0039 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 24/50\n",
      "97/97 [==============================] - 230s 2s/step - loss: 5.5491 - accuracy: 0.0025 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 25/50\n",
      "97/97 [==============================] - 229s 2s/step - loss: 5.5491 - accuracy: 0.0023 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 26/50\n",
      "97/97 [==============================] - 225s 2s/step - loss: 5.5491 - accuracy: 0.0037 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 27/50\n",
      "97/97 [==============================] - 227s 2s/step - loss: 5.5491 - accuracy: 0.0038 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 28/50\n",
      "97/97 [==============================] - 225s 2s/step - loss: 5.5491 - accuracy: 0.0029 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 29/50\n",
      "97/97 [==============================] - 230s 2s/step - loss: 5.5491 - accuracy: 0.0038 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 30/50\n",
      "97/97 [==============================] - 228s 2s/step - loss: 5.5491 - accuracy: 0.0043 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 31/50\n",
      "97/97 [==============================] - 232s 2s/step - loss: 5.5496 - accuracy: 0.0029 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 32/50\n",
      "97/97 [==============================] - 249s 3s/step - loss: 5.5491 - accuracy: 0.0041 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 33/50\n",
      "97/97 [==============================] - 2071s 22s/step - loss: 5.5491 - accuracy: 0.0042 - val_loss: 5.5491 - val_accuracy: 0.0029\n",
      "Epoch 34/50\n",
      "70/97 [====================>.........] - ETA: 1:02 - loss: 5.5491 - accuracy: 0.0035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-25c79b9a810e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m metrics=['accuracy'])\n\u001b[1;32m----> 4\u001b[1;33m history = model_cnn.fit(x_train,y_train,\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "def get_compiled_model():\n",
    "    build_model(257)\n",
    "\n",
    "def make_or_restore_model():\n",
    "    #either restore the latest model or create a fresh one if theres no checkpoint available\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints,key=os.path.getctime)\n",
    "        print(\"restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "def run_training():\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = make_or_restore_model()\n",
    "    callbacks = [\n",
    "        # this callback saves a saved model every epoch\n",
    "        # we can include the current epoch in the folder name\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_dir + \"/ckpt-{epoch}\",save_freq=\"epoch\"\n",
    "        )\n",
    "    ]\n",
    "    model_cnn.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "    history = model_cnn.fit(x_train,y_train,\n",
    "    batch_size=256,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(x_test,y_test))\n",
    "\n",
    "\n",
    "#the first run creates the model\n",
    "run_training()\n",
    "\n",
    "#the second time you call it will pick off from the last saved point\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}